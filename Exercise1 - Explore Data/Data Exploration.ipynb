{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ff70760f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products columns: Index(['CATEGORY_1', 'CATEGORY_2', 'CATEGORY_3', 'CATEGORY_4', 'MANUFACTURER',\n",
      "       'BRAND', 'BARCODE'],\n",
      "      dtype='object')\n",
      "Transactions columns: Index(['RECEIPT_ID', 'PURCHASE_DATE', 'SCAN_DATE', 'STORE_NAME', 'USER_ID',\n",
      "       'BARCODE', 'FINAL_QUANTITY', 'FINAL_SALE'],\n",
      "      dtype='object')\n",
      "Users columns: Index(['ID', 'CREATED_DATE', 'BIRTH_DATE', 'STATE', 'LANGUAGE', 'GENDER'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "try:\n",
    "    products = pd.read_csv('/Users/nani/Desktop/Fetch/PRODUCTS_TAKEHOME.csv')\n",
    "    transactions = pd.read_csv('/Users/nani/Desktop/Fetch/TRANSACTION_TAKEHOME.csv')\n",
    "    users = pd.read_csv('/Users/nani/Desktop/Fetch/USER_TAKEHOME.csv')\n",
    "    print(f\"Products columns: {products.columns}\")\n",
    "    print(f\"Transactions columns: {transactions.columns}\")\n",
    "    print(f\"Users columns: {users.columns}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: One or more CSV files not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9b2ce98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " PRODUCTS_TAKEHOME.csv \n",
      " (rows, columns): (845552, 7)\n",
      "Missing values:\n",
      " CATEGORY_1         111\n",
      "CATEGORY_2        1424\n",
      "CATEGORY_3       60566\n",
      "CATEGORY_4      778093\n",
      "MANUFACTURER    226474\n",
      "BRAND           226472\n",
      "BARCODE           4025\n",
      "dtype: int64 \n",
      "\n",
      "Duplicate rows: 215\n",
      "Column 'CATEGORY_1' missing entries: 111\n",
      "Column 'CATEGORY_2' missing entries: 1424\n",
      "Column 'CATEGORY_3' missing entries: 60566\n",
      "Column 'CATEGORY_4' missing entries: 778093\n",
      "Column 'MANUFACTURER' missing entries: 226474\n",
      "Column 'BRAND' missing entries: 226472\n",
      "Column 'BARCODE' missing entries: 4025\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "print(\"\\n PRODUCTS_TAKEHOME.csv \")\n",
    "print(\" (rows, columns):\", products.shape)\n",
    "print(\"Missing values:\\n\", products.isnull().sum(), \"\\n\")\n",
    "print(\"Duplicate rows:\", products.duplicated().sum())\n",
    "product_fields = [\"CATEGORY_1\",\"CATEGORY_2\",\"CATEGORY_3\", \"CATEGORY_4\", \"MANUFACTURER\", \"BRAND\", \"BARCODE\"]\n",
    "for col in product_fields:\n",
    "    missing = products[col].isnull().sum()\n",
    "    print(f\"Column '{col}' missing entries: {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4b6af857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRANSACTION_TAKEHOME.csv \n",
      " (rows, columns): (50000, 8)\n",
      "Missing values:\n",
      " RECEIPT_ID           0\n",
      "PURCHASE_DATE        0\n",
      "SCAN_DATE            0\n",
      "STORE_NAME           0\n",
      "USER_ID              0\n",
      "BARCODE           5762\n",
      "FINAL_QUANTITY       0\n",
      "FINAL_SALE           0\n",
      "dtype: int64 \n",
      "\n",
      "Duplicate rows: 171\n",
      "BARCODE missing entries: 5762\n",
      "Data type of 'FINAL_SALE': object\n",
      "Data type of 'FINAL_QUANTITY': object\n",
      "Unique values in 'FINAL_SALE': [' ' '1.49' '3.49' ... '11.02' '20.17' '42.38']\n",
      "Unique values in 'FINAL_QUANTITY': ['1.00' 'zero' '2.00' '3.00' '4.00' '4.55' '2.83' '2.34' '0.46' '7.00'\n",
      " '18.00' '12.00' '5.00' '2.17' '0.23' '8.00' '1.35' '0.09' '2.58' '1.47'\n",
      " '16.00' '0.62' '1.24' '1.40' '0.51' '0.53' '1.69' '6.00' '2.39' '2.60'\n",
      " '10.00' '0.86' '1.54' '1.88' '2.93' '1.28' '0.65' '2.89' '1.44' '2.75'\n",
      " '1.81' '276.00' '0.87' '2.10' '3.33' '2.54' '2.20' '1.93' '1.34' '1.13'\n",
      " '2.19' '0.83' '2.61' '0.28' '1.50' '0.97' '0.24' '1.18' '6.22' '1.22'\n",
      " '1.23' '2.57' '1.07' '2.11' '0.48' '9.00' '3.11' '1.08' '5.53' '1.89'\n",
      " '0.01' '2.18' '1.99' '0.04' '2.25' '1.37' '3.02' '0.35' '0.99' '1.80'\n",
      " '3.24' '0.94' '2.04' '3.69' '0.70' '2.52' '2.27']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n TRANSACTION_TAKEHOME.csv \")\n",
    "print(\" (rows, columns):\", transactions.shape)\n",
    "print(\"Missing values:\\n\", transactions.isnull().sum(), \"\\n\")\n",
    "print(\"Duplicate rows:\", transactions.duplicated().sum())\n",
    "\n",
    "print(\"BARCODE missing entries:\", transactions[\"BARCODE\"].isnull().sum())\n",
    "\n",
    "print(\"Data type of 'FINAL_SALE':\", transactions[\"FINAL_SALE\"].dtype)\n",
    "print(\"Data type of 'FINAL_QUANTITY':\", transactions[\"FINAL_QUANTITY\"].dtype)\n",
    "\n",
    "print(\"Unique values in 'FINAL_SALE':\", transactions[\"FINAL_SALE\"].unique())\n",
    "print(\"Unique values in 'FINAL_QUANTITY':\", transactions[\"FINAL_QUANTITY\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "cb3c5565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== USER_TAKEHOME.csv ==========\n",
      " (rows, columns): (100000, 6)\n",
      "Missing values:\n",
      " ID                  0\n",
      "CREATED_DATE        0\n",
      "BIRTH_DATE       3675\n",
      "STATE            4812\n",
      "LANGUAGE        30508\n",
      "GENDER           5892\n",
      "dtype: int64 \n",
      "\n",
      "Duplicate rows: 0\n",
      "Column 'BIRTH_DATE' missing entries: 3675\n",
      "Column 'STATE' missing entries: 4812\n",
      "Column 'LANGUAGE' missing entries: 30508\n",
      "Column 'GENDER' missing entries: 5892\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n========== USER_TAKEHOME.csv ==========\")\n",
    "print(\" (rows, columns):\", users.shape)\n",
    "print(\"Missing values:\\n\", users.isnull().sum(), \"\\n\")\n",
    "print(\"Duplicate rows:\", users.duplicated().sum())\n",
    "\n",
    "user_fields = [\"BIRTH_DATE\", \"STATE\", \"LANGUAGE\", \"GENDER\"]\n",
    "for col in user_fields:\n",
    "    missing = users[col].isnull().sum()\n",
    "    print(f\"Column '{col}' missing entries: {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "fb74884b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Genders:\n",
      " female                    64240\n",
      "male                      25829\n",
      "NaN                        5892\n",
      "transgender                1772\n",
      "prefer_not_to_say          1350\n",
      "non_binary                  473\n",
      "unknown                     196\n",
      "not_listed                  180\n",
      "Non-Binary                   34\n",
      "not_specified                28\n",
      "My gender isn't listed        5\n",
      "Prefer not to say             1\n",
      "Name: GENDER, dtype: int64 \n",
      "\n",
      "User Languages:\n",
      " en        63403\n",
      "NaN       30508\n",
      "es-419     6089\n",
      "Name: LANGUAGE, dtype: int64 \n",
      "\n",
      "Transaction Store Names:\n",
      " WALMART                 21326\n",
      "DOLLAR GENERAL STORE     2748\n",
      "ALDI                     2640\n",
      "KROGER                   1494\n",
      "TARGET                   1484\n",
      "Name: STORE_NAME, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Value counts to understand categorical distributions\n",
    "print(\"User Genders:\\n\", df_users['GENDER'].value_counts(dropna=False), \"\\n\")\n",
    "print(\"User Languages:\\n\", df_users['LANGUAGE'].value_counts(dropna=False), \"\\n\")\n",
    "print(\"Transaction Store Names:\\n\", df_transactions['STORE_NAME'].value_counts(dropna=False).head(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9dc74811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Columns: ['CATEGORY_1', 'CATEGORY_2', 'CATEGORY_3', 'CATEGORY_4', 'MANUFACTURER', 'BRAND', 'BARCODE']\n",
      "Transaction Columns: ['RECEIPT_ID', 'PURCHASE_DATE', 'SCAN_DATE', 'STORE_NAME', 'USER_ID', 'BARCODE', 'FINAL_QUANTITY', 'FINAL_SALE']\n",
      "User Columns: ['ID', 'CREATED_DATE', 'BIRTH_DATE', 'STATE', 'LANGUAGE', 'GENDER']\n"
     ]
    }
   ],
   "source": [
    "print(\"Product Columns:\", products.columns.tolist())\n",
    "print(\"Transaction Columns:\", transactions.columns.tolist())\n",
    "print(\"User Columns:\", users.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6870bfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert potential date columns\n",
    "df_users['CREATED_DATE'] = pd.to_datetime(df_users['CREATED_DATE'], errors='coerce')\n",
    "df_users['BIRTH_DATE'] = pd.to_datetime(df_users['BIRTH_DATE'], errors='coerce')\n",
    "df_transactions['PURCHASE_DATE'] = pd.to_datetime(df_transactions['PURCHASE_DATE'], errors='coerce')\n",
    "df_transactions['SCAN_DATE'] = pd.to_datetime(df_transactions['SCAN_DATE'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c08d647f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types after conversion:\n",
      "CREATED_DATE: datetime64[ns, UTC]\n",
      "BIRTH_DATE: datetime64[ns, UTC]\n",
      "PURCHASE_DATE: datetime64[ns]\n",
      "SCAN_DATE: datetime64[ns, UTC]\n"
     ]
    }
   ],
   "source": [
    "print(\"Data types after conversion:\")\n",
    "print(\"CREATED_DATE:\", df_users['CREATED_DATE'].dtype)\n",
    "print(\"BIRTH_DATE:\", df_users['BIRTH_DATE'].dtype)\n",
    "print(\"PURCHASE_DATE:\", df_transactions['PURCHASE_DATE'].dtype)\n",
    "print(\"SCAN_DATE:\", df_transactions['SCAN_DATE'].dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "615a53f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Users - Sample Date Fields:\n",
      "                   CREATED_DATE                BIRTH_DATE\n",
      "31936 2020-03-06 23:11:09+00:00 1993-08-21 00:00:00+00:00\n",
      "63805 2024-03-28 20:16:51+00:00 1985-02-02 07:00:00+00:00\n",
      "15855 2019-02-16 23:27:16+00:00 1987-02-12 06:00:00+00:00\n",
      "71507 2021-04-13 21:56:45+00:00 1967-12-30 16:56:57+00:00\n",
      "53450 2022-01-23 02:45:41+00:00 1984-06-15 05:00:00+00:00\n",
      "\n",
      " Transactions - Sample Date Fields:\n",
      "      PURCHASE_DATE                        SCAN_DATE\n",
      "3611     2024-07-12 2024-07-17 12:14:06.171000+00:00\n",
      "33688    2024-07-17 2024-07-28 19:13:43.325000+00:00\n",
      "11707    2024-06-18 2024-06-21 12:17:35.694000+00:00\n",
      "40349    2024-07-18 2024-07-31 07:31:20.629000+00:00\n",
      "46360    2024-08-18 2024-08-18 12:16:35.920000+00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Users - Sample Date Fields:\")\n",
    "print(df_users[['CREATED_DATE', 'BIRTH_DATE']].sample(5))\n",
    "\n",
    "print(\"\\n Transactions - Sample Date Fields:\")\n",
    "print(df_transactions[['PURCHASE_DATE', 'SCAN_DATE']].sample(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4893d066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Null (NaT) values after conversion:\n",
      "CREATED_DATE invalid entries: 0\n",
      "BIRTH_DATE invalid entries: 3675\n",
      "PURCHASE_DATE invalid entries: 0\n",
      "SCAN_DATE invalid entries: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Null (NaT) values after conversion:\")\n",
    "print(\"CREATED_DATE invalid entries:\", df_users['CREATED_DATE'].isnull().sum())\n",
    "print(\"BIRTH_DATE invalid entries:\", df_users['BIRTH_DATE'].isnull().sum())\n",
    "print(\"PURCHASE_DATE invalid entries:\", df_transactions['PURCHASE_DATE'].isnull().sum())\n",
    "print(\"SCAN_DATE invalid entries:\", df_transactions['SCAN_DATE'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "69acdf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Date Ranges:\n",
      "Users CREATED_DATE: 2014-04-18 23:14:55+00:00 to 2024-09-11 17:59:15+00:00\n",
      "Users BIRTH_DATE: 1900-01-01 00:00:00+00:00 to 2022-04-03 07:00:00+00:00\n",
      "Transactions PURCHASE_DATE: 2024-06-12 00:00:00 to 2024-09-08 00:00:00\n",
      "Transactions SCAN_DATE: 2024-06-12 06:36:34.910000+00:00 to 2024-09-08 23:07:19.836000+00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Date Ranges:\")\n",
    "print(\"Users CREATED_DATE:\", df_users['CREATED_DATE'].min(), \"to\", df_users['CREATED_DATE'].max())\n",
    "print(\"Users BIRTH_DATE:\", df_users['BIRTH_DATE'].min(), \"to\", df_users['BIRTH_DATE'].max())\n",
    "print(\"Transactions PURCHASE_DATE:\", df_transactions['PURCHASE_DATE'].min(), \"to\", df_transactions['PURCHASE_DATE'].max())\n",
    "print(\"Transactions SCAN_DATE:\", df_transactions['SCAN_DATE'].min(), \"to\", df_transactions['SCAN_DATE'].max())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
